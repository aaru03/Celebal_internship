{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25158e2e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecca44ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard',\n",
      "       'Parents/Children Aboard', 'Fare'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass                                               Name  \\\n",
      "0         0       3                             Mr. Owen Harris Braund   \n",
      "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
      "2         1       3                              Miss. Laina Heikkinen   \n",
      "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
      "4         0       3                            Mr. William Henry Allen   \n",
      "\n",
      "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
      "0    male  22.0                        1                        0   7.2500  \n",
      "1  female  38.0                        1                        0  71.2833  \n",
      "2  female  26.0                        0                        0   7.9250  \n",
      "3  female  35.0                        1                        0  53.1000  \n",
      "4    male  35.0                        0                        0   8.0500  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "print(data.columns)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d1e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933fbae",
   "metadata": {},
   "source": [
    "Step 1: Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13908bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate numerical and categorical columns\n",
    "numerical_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3181afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Survived' in numerical_cols:\n",
    "    numerical_cols.remove('Survived')\n",
    "if 'Survived' in categorical_cols:\n",
    "    categorical_cols.remove('Survived')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e30bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f214a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_cols] = numerical_imputer.fit_transform(data[numerical_cols])\n",
    "data[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090dfa3",
   "metadata": {},
   "source": [
    "Step 2: Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57558589",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False, drop='first')  # drop='first' to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57733262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer to apply transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),  # Scale numerical columns\n",
    "        ('cat', onehot, categorical_cols)  # One-hot encode categorical columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad26edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived', axis=1)  # 'Survived' is the target variable in the Titanic dataset\n",
    "y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e4e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_preprocessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d3919",
   "metadata": {},
   "source": [
    "Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd55ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['NewFeature'] = data['Age'] + data['Fare']\n",
    "X['NewFeature'] = StandardScaler().fit_transform(X[['NewFeature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9416be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = np.hstack([X_preprocessed, X[['NewFeature']].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e6373",
   "metadata": {},
   "source": [
    "Step 4: Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2332ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f968fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# Now, X_train and X_test are ready to be used for training and testing your machine learning model\n",
    "\n",
    "print(\"Preprocessing and feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f41799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (709, 893)\n",
      "Shape of X_test: (178, 893)\n",
      "Shape of y_train: (709,)\n",
      "Shape of y_test: (178,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
